{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "15ilZArFm1k0HxfA3mLrMDycOyoxRvEH_",
      "authorship_tag": "ABX9TyOwYQQQyjmZYdOFx79zZTtz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emrecan1923/yapayzekaprojelerim/blob/main/Hisse_Senedi_Tahmini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4XTT-4ZdfAsS",
        "outputId": "f2997fec-9fec-4f48-dd51-b911263256e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "13/13 [==============================] - 9s 19ms/step - loss: 0.0295\n",
            "Epoch 2/50\n",
            "13/13 [==============================] - 0s 21ms/step - loss: 0.0083\n",
            "Epoch 3/50\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 0.0048\n",
            "Epoch 4/50\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 0.0036\n",
            "Epoch 5/50\n",
            "13/13 [==============================] - 0s 28ms/step - loss: 0.0036\n",
            "Epoch 6/50\n",
            "13/13 [==============================] - 0s 32ms/step - loss: 0.0031\n",
            "Epoch 7/50\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 0.0035\n",
            "Epoch 8/50\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.0040\n",
            "Epoch 9/50\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 0.0032\n",
            "Epoch 10/50\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.0031\n",
            "Epoch 11/50\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.0032\n",
            "Epoch 12/50\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.0031\n",
            "Epoch 13/50\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.0031\n",
            "Epoch 14/50\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.0030\n",
            "Epoch 15/50\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.0032\n",
            "Epoch 16/50\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.0030\n",
            "Epoch 17/50\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 0.0030\n",
            "Epoch 18/50\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.0028\n",
            "Epoch 19/50\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.0025\n",
            "Epoch 20/50\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.0026\n",
            "Epoch 21/50\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 0.0027\n",
            "Epoch 22/50\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.0027\n",
            "Epoch 23/50\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.0024\n",
            "Epoch 24/50\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.0026\n",
            "Epoch 25/50\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 0.0026\n",
            "Epoch 26/50\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.0028\n",
            "Epoch 27/50\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.0026\n",
            "Epoch 28/50\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.0024\n",
            "Epoch 29/50\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.0025\n",
            "Epoch 30/50\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.0022\n",
            "Epoch 31/50\n",
            "13/13 [==============================] - 0s 19ms/step - loss: 0.0021\n",
            "Epoch 32/50\n",
            "13/13 [==============================] - 0s 18ms/step - loss: 0.0024\n",
            "Epoch 33/50\n",
            "13/13 [==============================] - 0s 21ms/step - loss: 0.0021\n",
            "Epoch 34/50\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 0.0022\n",
            "Epoch 35/50\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.0019\n",
            "Epoch 36/50\n",
            "13/13 [==============================] - 0s 29ms/step - loss: 0.0019\n",
            "Epoch 37/50\n",
            "13/13 [==============================] - 0s 29ms/step - loss: 0.0021\n",
            "Epoch 38/50\n",
            "13/13 [==============================] - 0s 29ms/step - loss: 0.0020\n",
            "Epoch 39/50\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.0021\n",
            "Epoch 40/50\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.0023\n",
            "Epoch 41/50\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.0018\n",
            "Epoch 42/50\n",
            "13/13 [==============================] - 0s 29ms/step - loss: 0.0017\n",
            "Epoch 43/50\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.0018\n",
            "Epoch 44/50\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.0016\n",
            "Epoch 45/50\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.0018\n",
            "Epoch 46/50\n",
            "13/13 [==============================] - 0s 31ms/step - loss: 0.0019\n",
            "Epoch 47/50\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.0019\n",
            "Epoch 48/50\n",
            "13/13 [==============================] - 0s 30ms/step - loss: 0.0017\n",
            "Epoch 49/50\n",
            "13/13 [==============================] - 0s 26ms/step - loss: 0.0019\n",
            "Epoch 50/50\n",
            "13/13 [==============================] - 0s 20ms/step - loss: 0.0018\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "Bugünkü Fiyat: 38.459999084472656\n",
            "Tahminler: [[36.130436]]\n"
          ]
        }
      ],
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Verileri almak için hisse senedi kodunu ve tarih aralığını belirleyin\n",
        "hisse_kodu = 'NETAS.IS'  # Netaş'ın BIST kodu\n",
        "veri_baslangic = '2022-01-01'  # Son 1 yılın verilerini kullan\n",
        "\n",
        "# Hisse senedi verilerini alın\n",
        "veriler = yf.download(hisse_kodu, start=veri_baslangic, progress=False)\n",
        "\n",
        "# Kapanış fiyatlarını alın\n",
        "kapanis_fiyatlari = veriler['Close'].values.reshape(-1, 1)\n",
        "\n",
        "# Verileri normalize etmek için MinMaxScaler kullanın\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "kapanis_fiyatlari_olcekli = scaler.fit_transform(kapanis_fiyatlari)\n",
        "\n",
        "# Zaman serisi pencelerini oluşturun (10 günlük pencere boyutu)\n",
        "pencere_boyutu = 10\n",
        "\n",
        "def zaman_serisi_pencereleri_olustur(veri, pencere_boyutu):\n",
        "    x = []\n",
        "    y = []\n",
        "    for i in range(len(veri) - pencere_boyutu - 1):\n",
        "        x.append(veri[i:(i + pencere_boyutu), 0])\n",
        "        y.append(veri[i + pencere_boyutu, 0])\n",
        "    return np.array(x), np.array(y)\n",
        "\n",
        "# Verileri eğitim ve test olarak bölelim\n",
        "egitim_orani = 0.8\n",
        "egitim_veri_sayisi = int(len(kapanis_fiyatlari_olcekli) * egitim_orani)\n",
        "\n",
        "egitim_verisi = kapanis_fiyatlari_olcekli[:egitim_veri_sayisi]\n",
        "test_verisi = kapanis_fiyatlari_olcekli[egitim_veri_sayisi:]\n",
        "\n",
        "egitim_x, egitim_y = zaman_serisi_pencereleri_olustur(egitim_verisi, pencere_boyutu)\n",
        "test_x, test_y = zaman_serisi_pencereleri_olustur(test_verisi, pencere_boyutu)\n",
        "\n",
        "# LSTM modelini oluşturun\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, return_sequences=True, input_shape=(pencere_boyutu, 1)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=50, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=50))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Modeli eğit\n",
        "model.fit(egitim_x, egitim_y, epochs=50, batch_size=32)\n",
        "\n",
        "# Bugünkü fiyatı tahmin etmek için kullanılacak veri\n",
        "bugunku_veri = kapanis_fiyatlari_olcekli[-pencere_boyutu:]\n",
        "\n",
        "# Tahminleri yap\n",
        "tahminler = []\n",
        "for i in range(1):  # Yarın için tahmin yapacak\n",
        "    tahmin = model.predict(bugunku_veri.reshape(1, pencere_boyutu, 1))\n",
        "    tahminler.append(tahmin[0, 0])  # Tahminleri listeye ekle\n",
        "    bugunku_veri = np.append(bugunku_veri[1:], tahmin)  # Yeni tahmini veriye ekle\n",
        "\n",
        "# Gerçek veriler ile karşılaştırmak için tahminlerin ölçeklemesini geri dönüştür\n",
        "tahminler = np.array(tahminler).reshape(-1, 1)\n",
        "tahminler_gercek_skala = scaler.inverse_transform(tahminler)\n",
        "\n",
        "# Bugünkü fiyat ve bir önceki günün fiyatını gerçek verilerden alın\n",
        "bugunku_veriler = yf.download(hisse_kodu, period=\"2d\")  # Bugünkü ve bir önceki gün\n",
        "bugunku_kapanis_fiyati = bugunku_veriler['Close'].iloc[-1]\n",
        "\n",
        "gercek_test_veriler = yf.download(hisse_kodu, start=veri_baslangic, end=bugunku_veriler.index[-2])\n",
        "\n",
        "# Gerçek test verilerinden kapanış fiyatlarını alın\n",
        "gercek_test_kapanis_fiyatlari = gercek_test_veriler['Close'].values.reshape(-1, 1)\n",
        "\n",
        "# Gerçek test kapanış fiyatlarını oluştur\n",
        "gercek_test_y = gercek_test_kapanis_fiyatlari[-2:]\n",
        "\n",
        "print(f\"Bugünkü Fiyat: {bugunku_kapanis_fiyati}\")\n",
        "# Tahminleri yazdır\n",
        "print(f\"Tahminler: {tahminler_gercek_skala}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veri Alımı: yfinance kütüphanesi aracılığıyla belirtilen hisse senedi kodundan ve tarih aralığından (1 yıllık veri) hisse senedi verilerini alır.\n",
        "\n",
        "Veri Ön İşleme: Alınan kapanış fiyatları, veri normalizasyonu için MinMaxScaler kullanılarak ölçeklenir.\n",
        "\n",
        "Zaman Serisi Pencereleri Oluşturma: Belirli bir pencere boyutuna göre veriler, zaman serisi pencereciklerine bölünür. Bu pencerecikler, girdi ve çıktı verileri olarak ayrılır. Burada LSTM modeline veri sağlamak için kullanılacak format oluşturulur.\n",
        "\n",
        "Eğitim ve Test Verisi Ayrımı: Veriler, belirli bir oranda (örneğin, %80) eğitim ve geri kalan kısmı test verisi olarak ayrılır.\n",
        "\n",
        "LSTM Modeli Oluşturma: Birkaç LSTM katmanı, dropout katmanları ve yoğun (Dense) katmanlar ile bir sinir ağı modeli oluşturulur. Bu modelin derlenmesi ve eğitilmesi gerçekleştirilir.\n",
        "\n",
        "Tahminlerin Yapılması: Eğitilen model, test verisi üzerinde tahminler yapar. Bugünkü fiyatı tahmin etmek için en son veriler kullanılır. İki gün için tahmin yapılır.\n",
        "\n",
        "Gerçek Verilerle Karşılaştırma: Gerçek verilerle yapılan tahminler arasındaki farklılıklar, ölçeklendirme işlemi kullanılarak hesaplanır.\n",
        "Modelin Kaydedilmesi: Eğitilen model, belirtilen dosya adıyla kaydedilir (hisseyi_tahmin_et_modeli.h5)."
      ],
      "metadata": {
        "id": "KrPnIpFa5qf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Hisseden veri al\n",
        "hisse_kodu = 'NETAS.IS'  # Netaş'ın BIST kodu\n",
        "veri_baslangic = '2022-01-01'  # Son 1 yılın verilerini kullan\n",
        "\n",
        "veriler = yf.download(hisse_kodu, start=veri_baslangic, progress=False)\n",
        "kapanis_fiyatlari = veriler['Close'].values.reshape(-1, 1)\n",
        "\n",
        "# Verileri normalize et\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "kapanis_fiyatlari_olcekli = scaler.fit_transform(kapanis_fiyatlari)\n",
        "\n",
        "# Modeli yükle\n",
        "model = load_model(\"/content/drive/MyDrive/hisseyi_tahmin_et_modeli.h5\")\n",
        "\n",
        "# Son 10 günü kullanarak tahmin yap\n",
        "pencere_boyutu = 10\n",
        "son_veri = kapanis_fiyatlari_olcekli[-pencere_boyutu:]\n",
        "son_veri = son_veri.reshape((1, pencere_boyutu, 1))\n",
        "\n",
        "tahmin = model.predict(son_veri)\n",
        "tahmin = scaler.inverse_transform(tahmin)\n",
        "\n",
        "bugunku_veri = yf.download(hisse_kodu, period=\"1d\")\n",
        "bugunku_kapanis_fiyati = bugunku_veri['Close'].iloc[-1]\n",
        "\n",
        "print(f\"Gelecek Gün Tahmini: {tahmin[0, 0]}\")\n",
        "print(f\"Bugünkü Fiyat: {bugunku_kapanis_fiyati}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOwRlEOZ4lXj",
        "outputId": "a844a0ff-cca3-40d8-93fb-20c871a112bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "[*********************100%%**********************]  1 of 1 completed\n",
            "Gelecek Gün Tahmini: 36.2808723449707\n",
            "Bugünkü Fiyat: 38.459999084472656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "yfinance kütüphanesi aracılığıyla belirli bir hisse senedinin (burada NETAS.IS - Netaş'ın BIST kodu) fiyat verilerini çeker.\n",
        "Hisse senedi kapanış fiyatlarını alır ve bu verileri kullanarak bir zaman serisi tahmini yapmak için bir yapay sinir ağı modeli kullanır.\n",
        "Verileri normalize eder. Normalizasyon, verilerin belirli bir aralığa ölçeklenmesini sağlar (burada 0 ile 1 arasında).\n",
        "Önceden eğitilmiş bir yapay sinir ağı modelini (hisseyi_tahmin_et_modeli.h5) yükler.\n",
        "Son 10 günün kapanış fiyatları verisi üzerinden gelecek bir günün fiyatını tahmin eder.\n",
        "Tahmin edilen fiyatı geri dönüştürerek, ölçeklendirme işlemi öncesindeki orijinal değerine dönüştürür.\n",
        "Güncel hisse senedi fiyatını çeker (yfinance kullanarak).\n",
        "Tahmin edilen fiyatı ve güncel fiyatı ekrana yazdırır."
      ],
      "metadata": {
        "id": "PFDZM9LQviBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!lscpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgTtVkL9rplh",
        "outputId": "84aa47c5-1121-4a23-d764-f5ab822590f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:            x86_64\n",
            "  CPU op-mode(s):        32-bit, 64-bit\n",
            "  Address sizes:         46 bits physical, 48 bits virtual\n",
            "  Byte Order:            Little Endian\n",
            "CPU(s):                  2\n",
            "  On-line CPU(s) list:   0,1\n",
            "Vendor ID:               GenuineIntel\n",
            "  Model name:            Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "    CPU family:          6\n",
            "    Model:               79\n",
            "    Thread(s) per core:  2\n",
            "    Core(s) per socket:  1\n",
            "    Socket(s):           1\n",
            "    Stepping:            0\n",
            "    BogoMIPS:            4400.33\n",
            "    Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clf\n",
            "                         lush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_\n",
            "                         good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fm\n",
            "                         a cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hyp\n",
            "                         ervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsb\n",
            "                         ase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsa\n",
            "                         veopt arat md_clear arch_capabilities\n",
            "Virtualization features: \n",
            "  Hypervisor vendor:     KVM\n",
            "  Virtualization type:   full\n",
            "Caches (sum of all):     \n",
            "  L1d:                   32 KiB (1 instance)\n",
            "  L1i:                   32 KiB (1 instance)\n",
            "  L2:                    256 KiB (1 instance)\n",
            "  L3:                    55 MiB (1 instance)\n",
            "NUMA:                    \n",
            "  NUMA node(s):          1\n",
            "  NUMA node0 CPU(s):     0,1\n",
            "Vulnerabilities:         \n",
            "  Gather data sampling:  Not affected\n",
            "  Itlb multihit:         Not affected\n",
            "  L1tf:                  Mitigation; PTE Inversion\n",
            "  Mds:                   Vulnerable; SMT Host state unknown\n",
            "  Meltdown:              Vulnerable\n",
            "  Mmio stale data:       Vulnerable\n",
            "  Retbleed:              Vulnerable\n",
            "  Spec rstack overflow:  Not affected\n",
            "  Spec store bypass:     Vulnerable\n",
            "  Spectre v1:            Vulnerable: __user pointer sanitization and usercopy barriers only; no swap\n",
            "                         gs barriers\n",
            "  Spectre v2:            Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Not affected\n",
            "  Srbds:                 Not affected\n",
            "  Tsx async abort:       Vulnerable\n"
          ]
        }
      ]
    }
  ]
}